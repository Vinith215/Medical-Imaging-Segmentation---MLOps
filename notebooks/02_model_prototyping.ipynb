{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f4fb51",
   "metadata": {},
   "source": [
    "### 1: Setup and Hyperparameter Preview\n",
    "\n",
    "We load the configs to ensure the notebook matches the eventual script-based pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.spatial.dictionary Orientationd.__init__:labels: Current default value of argument `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` was changed in version None from `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` to `labels=None`. Default value changed to None meaning that the transform now uses the 'space' of a meta-tensor, if applicable, to determine appropriate axis labels.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ModelConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelConfig\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransform_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransformConfig\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelFactory\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Set device\u001b[39;00m\n\u001b[32m     17\u001b[39m device = torch.device(GlobalConfig.DEVICE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\Medical-Imaging-Segmentation\\notebooks\\..\\src\\model_factory.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmonai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlosses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DiceLoss\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmonai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DiceMetric\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mSegmentationFactory\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;129;43m@staticmethod\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mget_model\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mModelConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSwinUNETR\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\Medical-Imaging-Segmentation\\notebooks\\..\\src\\model_factory.py:8\u001b[39m, in \u001b[36mSegmentationFactory\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSegmentationFactory\u001b[39;00m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_model\u001b[39m(config: \u001b[43mModelConfig\u001b[49m, device):\n\u001b[32m      9\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m config.model_type == \u001b[33m\"\u001b[39m\u001b[33mSwinUNETR\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     10\u001b[39m             model = SwinUNETR(\n\u001b[32m     11\u001b[39m                 img_size=config.img_size,\n\u001b[32m     12\u001b[39m                 in_channels=config.in_channels,\n\u001b[32m     13\u001b[39m                 out_channels=config.out_channels,\n\u001b[32m     14\u001b[39m                 feature_size=\u001b[32m48\u001b[39m\n\u001b[32m     15\u001b[39m             ).to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'ModelConfig' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.data import DataLoader, Dataset, decollate_batch\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from config.global_config import GlobalConfig\n",
    "from config.model_config import ModelConfig\n",
    "from config.transform_config import TransformConfig\n",
    "from src.model_factory import ModelFactory\n",
    "\n",
    "# Set device\n",
    "device = torch.device(GlobalConfig.DEVICE)\n",
    "print(f\"üöÄ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae4d5f5",
   "metadata": {},
   "source": [
    "### 2: Mock Dataset Creation\n",
    "\n",
    "For prototyping, we don't want to load 100GB of data. We select 4-8 samples to verify the gradient flow and transform logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Load paths from processed directory\n",
    "images = sorted(glob.glob(str(GlobalConfig.PROCESSED_DATA_DIR / \"*_img.nii.gz\")))\n",
    "labels = sorted(glob.glob(str(GlobalConfig.PROCESSED_DATA_DIR / \"*_seg.nii.gz\")))\n",
    "\n",
    "data_dicts = [{\"image\": i, \"label\": l} for i, l in zip(images, labels)]\n",
    "\n",
    "# Split for prototype (e.g., 4 train, 2 val)\n",
    "train_files, val_files = data_dicts[:4], data_dicts[4:6]\n",
    "\n",
    "train_ds = Dataset(data=train_files, transform=TransformConfig.get_train_transforms())\n",
    "train_loader = DataLoader(train_ds, batch_size=ModelConfig.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_ds = Dataset(data=val_files, transform=TransformConfig.get_val_transforms())\n",
    "val_loader = DataLoader(val_ds, batch_size=1)\n",
    "\n",
    "print(f\"üì¶ Prototype ready: {len(train_ds)} train, {len(val_ds)} val samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6c814",
   "metadata": {},
   "source": [
    "### 3: Sanity Check - Visualizing Transforms\n",
    "Before training, we must visualize the 3D patches. If the \"organ\" is missing from the random crop, the model won't learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980fd2c0",
   "metadata": {},
   "source": [
    "check_ds = Dataset(data=train_files, transform=TransformConfig.get_train_transforms())\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "check_data = next(iter(check_loader))\n",
    "\n",
    "image, label = (check_data[\"image\"][0][0], check_data[\"label\"][0][0])\n",
    "print(f\"Image shape: {image.shape}, Label shape: {label.shape}\")\n",
    "\n",
    "# Plot center slice of the 3D patch\n",
    "plt.figure(\"check\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Image Patch (Center Slice)\")\n",
    "plt.imshow(image[:, :, image.shape[2]//2], cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Label Patch (Center Slice)\")\n",
    "plt.imshow(label[:, :, label.shape[2]//2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9f2b8",
   "metadata": {},
   "source": [
    "### 4: Model Initialization\n",
    "We instantiate the Swin-UNETR. This is a transformer-based architecture that uses shifted windows for 3D context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab15b1",
   "metadata": {},
   "source": [
    "model = ModelFactory.get_model(ModelConfig.MODEL_NAME).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "from monai.losses import DiceFocalLoss\n",
    "loss_function = DiceFocalLoss(to_onehot_y=ModelConfig.OUT_CHANNELS, softmax=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=ModelConfig.LEARNING_RATE)\n",
    "\n",
    "print(f\"üèóÔ∏è {ModelConfig.MODEL_NAME} initialized with {sum(p.numel() for p in model.parameters()):,} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce02aa",
   "metadata": {},
   "source": [
    "### 5: The \"Overfit\" Test\n",
    "In prototyping, the goal is to see if the model can overfit on 2 images. If it can't reduce loss to near zero on a tiny set, there is a bug in the code or the transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b30425",
   "metadata": {},
   "source": [
    "max_epochs = 50\n",
    "epoch_loss_values = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_data in train_loader:\n",
    "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss /= len(train_loader)\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs} | Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "plt.plot(epoch_loss_values)\n",
    "plt.title(\"Prototype Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d90203",
   "metadata": {},
   "source": [
    "### 6: Prototype Inference (Sliding Window)\n",
    "We use sliding_window_inference because the full volume won't fit in VRAM. This tests the logic that will eventually live in src/inference.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553a4a4",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for val_data in val_loader:\n",
    "        val_inputs = val_data[\"image\"].to(device)\n",
    "        roi_size = ModelConfig.ROI_SIZE\n",
    "        sw_batch_size = 4\n",
    "        \n",
    "        val_outputs = sliding_window_inference(\n",
    "            val_inputs, roi_size, sw_batch_size, model, overlap=ModelConfig.OVERLAP\n",
    "        )\n",
    "        \n",
    "        # Visualize the result\n",
    "        plt.figure(\"Inference\", (18, 6))\n",
    "        plt.subplot(1, 3, 1); plt.title(\"Input\"); plt.imshow(val_inputs.cpu()[0, 0, :, :, 64], cmap=\"gray\")\n",
    "        plt.subplot(1, 3, 2); plt.title(\"Ground Truth\"); plt.imshow(val_data[\"label\"][0, 0, :, :, 64])\n",
    "        plt.subplot(1, 3, 3); plt.title(\"Prediction\"); plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, 64])\n",
    "        plt.show()\n",
    "        break # Just show one for the prototype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
